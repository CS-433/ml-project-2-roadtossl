{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(pos_file, neg_file):\n",
    "    \"\"\"\n",
    "    Load positive and negative vectors, combine and shuffle them.\n",
    "    \n",
    "    Parameters:\n",
    "    pos_file: str\n",
    "        Path to file containing positive vectors.\n",
    "    neg_file: str\n",
    "        Path to file containing negative vectors.\n",
    "    \n",
    "    Returns:\n",
    "    X: np.array\n",
    "        Combined and shuffled vectors.\n",
    "    y: np.array\n",
    "        Labels for the vectors.\n",
    "    \"\"\"\n",
    "    # Read files\n",
    "    pos_vectors = np.loadtxt(pos_file)\n",
    "    neg_vectors = np.loadtxt(neg_file)\n",
    "    \n",
    "    # Create labels\n",
    "    pos_labels = np.ones(len(pos_vectors))\n",
    "    print(pos_labels)\n",
    "    neg_labels = -np.ones(len(neg_vectors))\n",
    "    print(neg_labels)\n",
    "    \n",
    "    # Combine data\n",
    "    X = np.vstack((pos_vectors, neg_vectors))\n",
    "    y = np.concatenate((pos_labels, neg_labels))\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[-1. -1. -1. ... -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "X, y = load_vectors(\"data/twitter-datasets/train_pos_full_embedding.txt\", \"data/twitter-datasets/train_neg_full_embedding.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X, y, hidden_layers=(64, 32), max_iter=200):\n",
    "    \"\"\"Train MLPClassifier with progress tracking\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layers,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42,\n",
    "        solver='adam',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training with progress bar\n",
    "    print(\"Training neural network...\")\n",
    "    with tqdm(total=max_iter) as pbar:\n",
    "        def update_progress(iter_num, loss):\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f'Loss: {loss:.4f}')\n",
    "        \n",
    "        # Set custom callback\n",
    "        model._callback = update_progress\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_nn(model, tweet_vector):\n",
    "    \"\"\"Predict sentiment for a single tweet vector\"\"\"\n",
    "    return model.predict([tweet_vector])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64642539\n",
      "Iteration 2, loss = 0.63573893\n",
      "Iteration 3, loss = 0.63137808\n",
      "Iteration 4, loss = 0.62821997\n",
      "Iteration 5, loss = 0.62593251\n",
      "Iteration 6, loss = 0.62420099\n",
      "Iteration 7, loss = 0.62294746\n",
      "Iteration 8, loss = 0.62166313\n",
      "Iteration 9, loss = 0.62065148\n",
      "Iteration 10, loss = 0.61972772\n",
      "Iteration 11, loss = 0.61908306\n",
      "Iteration 12, loss = 0.61824276\n",
      "Iteration 13, loss = 0.61768044\n",
      "Iteration 14, loss = 0.61712394\n",
      "Iteration 15, loss = 0.61653890\n",
      "Iteration 16, loss = 0.61603562\n",
      "Iteration 17, loss = 0.61553594\n",
      "Iteration 18, loss = 0.61519534\n",
      "Iteration 19, loss = 0.61484738\n",
      "Iteration 20, loss = 0.61452720\n",
      "Iteration 21, loss = 0.61413354\n",
      "Iteration 22, loss = 0.61388404\n",
      "Iteration 23, loss = 0.61364018\n",
      "Iteration 24, loss = 0.61329764\n",
      "Iteration 25, loss = 0.61313312\n",
      "Iteration 26, loss = 0.61284476\n",
      "Iteration 27, loss = 0.61264726\n",
      "Iteration 28, loss = 0.61244147\n",
      "Iteration 29, loss = 0.61230406\n",
      "Iteration 30, loss = 0.61210717\n",
      "Iteration 31, loss = 0.61184224\n",
      "Iteration 32, loss = 0.61170311\n",
      "Iteration 33, loss = 0.61158883\n",
      "Iteration 34, loss = 0.61130769\n",
      "Iteration 35, loss = 0.61132101\n",
      "Iteration 36, loss = 0.61114940\n",
      "Iteration 37, loss = 0.61097923\n",
      "Iteration 38, loss = 0.61084391\n",
      "Iteration 39, loss = 0.61069682\n",
      "Iteration 40, loss = 0.61061351\n",
      "Iteration 41, loss = 0.61047992\n",
      "Iteration 42, loss = 0.61033123\n",
      "Iteration 43, loss = 0.61014477\n",
      "Iteration 44, loss = 0.61008533\n",
      "Iteration 45, loss = 0.61001309\n",
      "Iteration 46, loss = 0.60982544\n",
      "Iteration 47, loss = 0.60972235\n",
      "Iteration 48, loss = 0.60962961\n",
      "Iteration 49, loss = 0.60957537\n",
      "Iteration 50, loss = 0.60943021\n",
      "Iteration 51, loss = 0.60925710\n",
      "Iteration 52, loss = 0.60937085\n",
      "Iteration 53, loss = 0.60921977\n",
      "Iteration 54, loss = 0.60917976\n",
      "Iteration 55, loss = 0.60895937\n",
      "Iteration 56, loss = 0.60889612\n",
      "Iteration 57, loss = 0.60893269\n",
      "Iteration 58, loss = 0.60880031\n",
      "Iteration 59, loss = 0.60864015\n",
      "Iteration 60, loss = 0.60858092\n",
      "Iteration 61, loss = 0.60857061\n",
      "Iteration 62, loss = 0.60852180\n",
      "Iteration 63, loss = 0.60833943\n",
      "Iteration 64, loss = 0.60839698\n",
      "Iteration 65, loss = 0.60832496\n",
      "Iteration 66, loss = 0.60825684\n",
      "Iteration 67, loss = 0.60823624\n",
      "Iteration 68, loss = 0.60816240\n",
      "Iteration 69, loss = 0.60801116\n",
      "Iteration 70, loss = 0.60802869\n",
      "Iteration 71, loss = 0.60794362\n",
      "Iteration 72, loss = 0.60786741\n",
      "Iteration 73, loss = 0.60787028\n",
      "Iteration 74, loss = 0.60780973\n",
      "Iteration 75, loss = 0.60781044\n",
      "Iteration 76, loss = 0.60775467\n",
      "Iteration 77, loss = 0.60777552\n",
      "Iteration 78, loss = 0.60762511\n",
      "Iteration 79, loss = 0.60765036\n",
      "Iteration 80, loss = 0.60751296\n",
      "Iteration 81, loss = 0.60759704\n",
      "Iteration 82, loss = 0.60742135\n",
      "Iteration 83, loss = 0.60740035\n",
      "Iteration 84, loss = 0.60747254\n",
      "Iteration 85, loss = 0.60734520\n",
      "Iteration 86, loss = 0.60727469\n",
      "Iteration 87, loss = 0.60726351\n",
      "Iteration 88, loss = 0.60726523\n",
      "Iteration 89, loss = 0.60720742\n",
      "Iteration 90, loss = 0.60715139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [08:42<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 91, loss = 0.60709115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.65      0.63      0.64    250091\n",
      "         1.0       0.64      0.66      0.65    249909\n",
      "\n",
      "    accuracy                           0.65    500000\n",
      "   macro avg       0.65      0.65      0.65    500000\n",
      "weighted avg       0.65      0.65      0.65    500000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_neural_network(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'train_neural_network.<locals>.update_progress'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage_NN_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'train_neural_network.<locals>.update_progress'"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "with open(\"average_NN_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
