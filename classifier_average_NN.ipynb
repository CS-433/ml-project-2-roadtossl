{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(pos_file, neg_file):\n",
    "    \"\"\"\n",
    "    Load positive and negative vectors, combine and shuffle them.\n",
    "    \n",
    "    Parameters:\n",
    "    pos_file: str\n",
    "        Path to file containing positive vectors.\n",
    "    neg_file: str\n",
    "        Path to file containing negative vectors.\n",
    "    \n",
    "    Returns:\n",
    "    X: np.array\n",
    "        Combined and shuffled vectors.\n",
    "    y: np.array\n",
    "        Labels for the vectors.\n",
    "    \"\"\"\n",
    "    # Read files\n",
    "    pos_vectors = np.loadtxt(pos_file)\n",
    "    neg_vectors = np.loadtxt(neg_file)\n",
    "    \n",
    "    # Create labels\n",
    "    pos_labels = np.ones(len(pos_vectors))\n",
    "    print(pos_labels)\n",
    "    neg_labels = -np.ones(len(neg_vectors))\n",
    "    print(neg_labels)\n",
    "    \n",
    "    # Combine data\n",
    "    X = np.vstack((pos_vectors, neg_vectors))\n",
    "    y = np.concatenate((pos_labels, neg_labels))\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[-1. -1. -1. ... -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "X, y = load_vectors(\"data/twitter-datasets/train_pos_embedding.txt\", \"data/twitter-datasets/train_neg_embedding.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X, y, hidden_layers=(64, 32), max_iter=200):\n",
    "    \"\"\"Train MLPClassifier with progress tracking\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layers,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42,\n",
    "        solver='adam',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training with progress bar\n",
    "    print(\"Training neural network...\")\n",
    "    with tqdm(total=max_iter) as pbar:\n",
    "        def update_progress(iter_num, loss):\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f'Loss: {loss:.4f}')\n",
    "        \n",
    "        # Set custom callback\n",
    "        model._callback = update_progress\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_nn(model, tweet_vector):\n",
    "    \"\"\"Predict sentiment for a single tweet vector\"\"\"\n",
    "    return model.predict([tweet_vector])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66421367\n",
      "Iteration 2, loss = 0.65212917\n",
      "Iteration 3, loss = 0.64706856\n",
      "Iteration 4, loss = 0.64448409\n",
      "Iteration 5, loss = 0.64252011\n",
      "Iteration 6, loss = 0.64063967\n",
      "Iteration 7, loss = 0.63854473\n",
      "Iteration 8, loss = 0.63784463\n",
      "Iteration 9, loss = 0.63665243\n",
      "Iteration 10, loss = 0.63526054\n",
      "Iteration 11, loss = 0.63426296\n",
      "Iteration 12, loss = 0.63357941\n",
      "Iteration 13, loss = 0.63257226\n",
      "Iteration 14, loss = 0.63169847\n",
      "Iteration 15, loss = 0.63130753\n",
      "Iteration 16, loss = 0.63021910\n",
      "Iteration 17, loss = 0.62926693\n",
      "Iteration 18, loss = 0.62863642\n",
      "Iteration 19, loss = 0.62809115\n",
      "Iteration 20, loss = 0.62798907\n",
      "Iteration 21, loss = 0.62701000\n",
      "Iteration 22, loss = 0.62641326\n",
      "Iteration 23, loss = 0.62641157\n",
      "Iteration 24, loss = 0.62532381\n",
      "Iteration 25, loss = 0.62539447\n",
      "Iteration 26, loss = 0.62427723\n",
      "Iteration 27, loss = 0.62329792\n",
      "Iteration 28, loss = 0.62317620\n",
      "Iteration 29, loss = 0.62283862\n",
      "Iteration 30, loss = 0.62197917\n",
      "Iteration 31, loss = 0.62195319\n",
      "Iteration 32, loss = 0.62146935\n",
      "Iteration 33, loss = 0.62092599\n",
      "Iteration 34, loss = 0.62060313\n",
      "Iteration 35, loss = 0.62013934\n",
      "Iteration 36, loss = 0.61981867\n",
      "Iteration 37, loss = 0.61957528\n",
      "Iteration 38, loss = 0.61923612\n",
      "Iteration 39, loss = 0.61866917\n",
      "Iteration 40, loss = 0.61842591\n",
      "Iteration 41, loss = 0.61790496\n",
      "Iteration 42, loss = 0.61781227\n",
      "Iteration 43, loss = 0.61754701\n",
      "Iteration 44, loss = 0.61702638\n",
      "Iteration 45, loss = 0.61643120\n",
      "Iteration 46, loss = 0.61599713\n",
      "Iteration 47, loss = 0.61593642\n",
      "Iteration 48, loss = 0.61610087\n",
      "Iteration 49, loss = 0.61552534\n",
      "Iteration 50, loss = 0.61548062\n",
      "Iteration 51, loss = 0.61495413\n",
      "Iteration 52, loss = 0.61491744\n",
      "Iteration 53, loss = 0.61439991\n",
      "Iteration 54, loss = 0.61392153\n",
      "Iteration 55, loss = 0.61359197\n",
      "Iteration 56, loss = 0.61360126\n",
      "Iteration 57, loss = 0.61352344\n",
      "Iteration 58, loss = 0.61337743\n",
      "Iteration 59, loss = 0.61304484\n",
      "Iteration 60, loss = 0.61270486\n",
      "Iteration 61, loss = 0.61234410\n",
      "Iteration 62, loss = 0.61191270\n",
      "Iteration 63, loss = 0.61218378\n",
      "Iteration 64, loss = 0.61200781\n",
      "Iteration 65, loss = 0.61142400\n",
      "Iteration 66, loss = 0.61135996\n",
      "Iteration 67, loss = 0.61154259\n",
      "Iteration 68, loss = 0.61105701\n",
      "Iteration 69, loss = 0.61069952\n",
      "Iteration 70, loss = 0.61051078\n",
      "Iteration 71, loss = 0.61030545\n",
      "Iteration 72, loss = 0.61021511\n",
      "Iteration 73, loss = 0.60955005\n",
      "Iteration 74, loss = 0.60976372\n",
      "Iteration 75, loss = 0.60966059\n",
      "Iteration 76, loss = 0.60957705\n",
      "Iteration 77, loss = 0.60889306\n",
      "Iteration 78, loss = 0.60912309\n",
      "Iteration 79, loss = 0.60904405\n",
      "Iteration 80, loss = 0.60889213\n",
      "Iteration 81, loss = 0.60842866\n",
      "Iteration 82, loss = 0.60852633\n",
      "Iteration 83, loss = 0.60836293\n",
      "Iteration 84, loss = 0.60827923\n",
      "Iteration 85, loss = 0.60782712\n",
      "Iteration 86, loss = 0.60745834\n",
      "Iteration 87, loss = 0.60747033\n",
      "Iteration 88, loss = 0.60790593\n",
      "Iteration 89, loss = 0.60684691\n",
      "Iteration 90, loss = 0.60697665\n",
      "Iteration 91, loss = 0.60708615\n",
      "Iteration 92, loss = 0.60741289\n",
      "Iteration 93, loss = 0.60647312\n",
      "Iteration 94, loss = 0.60648674\n",
      "Iteration 95, loss = 0.60665608\n",
      "Iteration 96, loss = 0.60622430\n",
      "Iteration 97, loss = 0.60663163\n",
      "Iteration 98, loss = 0.60587359\n",
      "Iteration 99, loss = 0.60587734\n",
      "Iteration 100, loss = 0.60616681\n",
      "Iteration 101, loss = 0.60591735\n",
      "Iteration 102, loss = 0.60549060\n",
      "Iteration 103, loss = 0.60575705\n",
      "Iteration 104, loss = 0.60515824\n",
      "Iteration 105, loss = 0.60497312\n",
      "Iteration 106, loss = 0.60495765\n",
      "Iteration 107, loss = 0.60476032\n",
      "Iteration 108, loss = 0.60473566\n",
      "Iteration 109, loss = 0.60472408\n",
      "Iteration 110, loss = 0.60504285\n",
      "Iteration 111, loss = 0.60443068\n",
      "Iteration 112, loss = 0.60439984\n",
      "Iteration 113, loss = 0.60458931\n",
      "Iteration 114, loss = 0.60399452\n",
      "Iteration 115, loss = 0.60383405\n",
      "Iteration 116, loss = 0.60421952\n",
      "Iteration 117, loss = 0.60378778\n",
      "Iteration 118, loss = 0.60356193\n",
      "Iteration 119, loss = 0.60400618\n",
      "Iteration 120, loss = 0.60352949\n",
      "Iteration 121, loss = 0.60324705\n",
      "Iteration 122, loss = 0.60323532\n",
      "Iteration 123, loss = 0.60335449\n",
      "Iteration 124, loss = 0.60330021\n",
      "Iteration 125, loss = 0.60330961\n",
      "Iteration 126, loss = 0.60259204\n",
      "Iteration 127, loss = 0.60311599\n",
      "Iteration 128, loss = 0.60286946\n",
      "Iteration 129, loss = 0.60271856\n",
      "Iteration 130, loss = 0.60235742\n",
      "Iteration 131, loss = 0.60234477\n",
      "Iteration 132, loss = 0.60240211\n",
      "Iteration 133, loss = 0.60241690\n",
      "Iteration 134, loss = 0.60159351\n",
      "Iteration 135, loss = 0.60174567\n",
      "Iteration 136, loss = 0.60136945\n",
      "Iteration 137, loss = 0.60202503\n",
      "Iteration 138, loss = 0.60160940\n",
      "Iteration 139, loss = 0.60183935\n",
      "Iteration 140, loss = 0.60150323\n",
      "Iteration 141, loss = 0.60119053\n",
      "Iteration 142, loss = 0.60170507\n",
      "Iteration 143, loss = 0.60144812\n",
      "Iteration 144, loss = 0.60075720\n",
      "Iteration 145, loss = 0.60111122\n",
      "Iteration 146, loss = 0.60088736\n",
      "Iteration 147, loss = 0.60060604\n",
      "Iteration 148, loss = 0.60072416\n",
      "Iteration 149, loss = 0.60045075\n",
      "Iteration 150, loss = 0.60076935\n",
      "Iteration 151, loss = 0.60018410\n",
      "Iteration 152, loss = 0.60038874\n",
      "Iteration 153, loss = 0.60076079\n",
      "Iteration 154, loss = 0.59991024\n",
      "Iteration 155, loss = 0.60017056\n",
      "Iteration 156, loss = 0.60009032\n",
      "Iteration 157, loss = 0.59977878\n",
      "Iteration 158, loss = 0.59956435\n",
      "Iteration 159, loss = 0.59997482\n",
      "Iteration 160, loss = 0.59938862\n",
      "Iteration 161, loss = 0.59938053\n",
      "Iteration 162, loss = 0.59991070\n",
      "Iteration 163, loss = 0.59987126\n",
      "Iteration 164, loss = 0.59989631\n",
      "Iteration 165, loss = 0.59924466\n",
      "Iteration 166, loss = 0.59939820\n",
      "Iteration 167, loss = 0.59899833\n",
      "Iteration 168, loss = 0.59958747\n",
      "Iteration 169, loss = 0.59922827\n",
      "Iteration 170, loss = 0.59937159\n",
      "Iteration 171, loss = 0.59887509\n",
      "Iteration 172, loss = 0.59870024\n",
      "Iteration 173, loss = 0.59847877\n",
      "Iteration 174, loss = 0.59865063\n",
      "Iteration 175, loss = 0.59803812\n",
      "Iteration 176, loss = 0.59834754\n",
      "Iteration 177, loss = 0.59838045\n",
      "Iteration 178, loss = 0.59850068\n",
      "Iteration 179, loss = 0.59815189\n",
      "Iteration 180, loss = 0.59786132\n",
      "Iteration 181, loss = 0.59765171\n",
      "Iteration 182, loss = 0.59801820\n",
      "Iteration 183, loss = 0.59782050\n",
      "Iteration 184, loss = 0.59756965\n",
      "Iteration 185, loss = 0.59784637\n",
      "Iteration 186, loss = 0.59793013\n",
      "Iteration 187, loss = 0.59728688\n",
      "Iteration 188, loss = 0.59768996\n",
      "Iteration 189, loss = 0.59728249\n",
      "Iteration 190, loss = 0.59743964\n",
      "Iteration 191, loss = 0.59756892\n",
      "Iteration 192, loss = 0.59760935\n",
      "Iteration 193, loss = 0.59739945\n",
      "Iteration 194, loss = 0.59679428\n",
      "Iteration 195, loss = 0.59728593\n",
      "Iteration 196, loss = 0.59710185\n",
      "Iteration 197, loss = 0.59673989\n",
      "Iteration 198, loss = 0.59713165\n",
      "Iteration 199, loss = 0.59669683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolas/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/200 [01:46<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200, loss = 0.59675117\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.65      0.60      0.63     19968\n",
      "         1.0       0.63      0.68      0.65     20032\n",
      "\n",
      "    accuracy                           0.64     40000\n",
      "   macro avg       0.64      0.64      0.64     40000\n",
      "weighted avg       0.64      0.64      0.64     40000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_neural_network(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'train_neural_network.<locals>.update_progress'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage_NN_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'train_neural_network.<locals>.update_progress'"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "with open(\"average_NN_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
