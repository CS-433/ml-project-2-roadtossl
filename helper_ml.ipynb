{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenise and Pad sequences (Optionel, deja fait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the tokenizer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m, oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Convert texts to sequences\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_sequences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(train_texts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['clean_text'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "# Pad sequences\n",
    "max_length = 100\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "max_length = 100\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=20000, output_dim=128),\n",
    "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer :\n",
    "- Purpose: Converts word indices into dense vectors (embeddings).\n",
    "- Parameters:\n",
    "    - output_dim = 128 : Dimension of the embedding vectors. Each word will be represented as a 128-dimensional vector.\n",
    "    - input_length = max_length : Length of input sequences. Required because the model needs to know the shape of its input.\n",
    "\n",
    "#### LSTM Layer :\n",
    "- Purpose: Processes the embedded sequences and captures temporal dependencies.\n",
    "- Parameters:\n",
    "    - 128 : Number of units (dimensionality of the output space). This is the number of LSTM cells in the layer.\n",
    "    - dropout = 0.2 : Fraction of the input units to drop (regular dropout) to prevent overfitting.\n",
    "    - recurrent_dropout = 0.2 : Fraction of the recurrent units to drop (dropout on the connections between the recurrent units).\n",
    "\n",
    "#### Dense output layer :\n",
    "- Purpose: Outputs a probability between 0 and 1, indicating the sentiment.\n",
    "- Parameters:\n",
    "    - 1 : Single neuron because it's a binary classification problem.\n",
    "    - activation = 'sigmoid': Activation function that outputs values between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss = 'binary_crossentropy': Loss function suitable for binary classification tasks.\n",
    "- optimizer = 'adam': Adam optimizer is an efficient stochastic gradient descent method.\n",
    "- metrics = ['accuracy']: Specifies the metric to evaluate during training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_padded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain_padded\u001b[49m, train_labels, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      2\u001b[0m           validation_data\u001b[38;5;241m=\u001b[39m(val_padded, val_labels))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_padded' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(train_padded, train_labels, epochs=5, batch_size=32,\n",
    "          validation_data=(val_padded, val_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train_padded : Input data (padded sequences) for training.\n",
    "- train_labels : Corresponding labels (0 or 1) for training data.\n",
    "- epochs=5 : Number of times the model will cycle through the entire training dataset.\n",
    "- batch_size = 32  Number of samples per gradient update. Smaller batch sizes can lead to more stable gradient estimates but increase computation time.\n",
    "- validation_data=(val_padded, val_labels): Data on which to evaluate the model at the end of each epoch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
